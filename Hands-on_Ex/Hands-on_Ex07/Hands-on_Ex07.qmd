---
title: "Hands-on Exercise 8 & 9"
author: "Dexter Wan"
date: "October 11, 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  freeze: true
---

## Description

Learning to delineate homogeneous region by using geographically referenced multivariate data in R.

### Changelog

11 Oct 24: Completed Hands-on Exercise 8 up to 12.7.

15 Oct 24: Completed Hands-on Exercise 9 from 12.8-12.10

7 Nov 24: Updated from "Hands-on Exercise 7" to "Hands-on Exercise 8 & 9"

## Importing Data and setting up R Environment

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, ggpubr, cluster, factoextra, NbClust, heatmaply, corrplot, psych, tidyverse, GGally)
```

```{r}
#| eval: false
shan_sf = st_read(dsn = "data/geospatial", 
                  layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>% # %in% indicates that we only want certain values in column ST
  select(c(2:7))
shan_sf
```

```{r}
ict = read_csv("data/aspatial/Shan-ICT.csv")
```

```{r}
summary(ict)
```

We will now derive the penetration rate of each ICT variable.

```{r}
ict_derived = ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
summary(ict_derived)
```

## Exploratory Data Analysis (EDA)

EDA is useful to see the distribution of variables.

Histograms can identify the overall distribution (e.g. left skew)

```{r}
ggplot(data = ict_derived, 
       aes(x = `RADIO`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")
```

Boxplot can detect outliers

```{r}
ggplot(data = ict_derived,
       aes(x = `RADIO`)) +
  geom_boxplot(color = "black",
               fill = "orange")
```

We can also do this using the penetration rate

```{r}
ggplot(data = ict_derived, 
       aes(x = `RADIO_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")
```

```{r}
ggplot(data = ict_derived,
       aes(x = `RADIO_PR`)) +
  geom_boxplot(color = "black",
               fill = "orange")
```

We can plot multiple plots side-by-side using *ggarrange()*

```{r}
radio = ggplot(data = ict_derived, 
             aes(x =  `RADIO_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

tv = ggplot(data = ict_derived, 
             aes(x =  `TV_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

llphone = ggplot(data = ict_derived, 
             aes(x =  `LLPHONE_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

mphone = ggplot(data = ict_derived, 
             aes(x =  `MPHONE_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

computer = ggplot(data = ict_derived, 
             aes(x =  `COMPUTER_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

internet = ggplot(data = ict_derived, 
             aes(x =  `INTERNET_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")
```

```{r}
#| fig-width: 12
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, nrow = 2)
```

Now lets join the data into a single sf file

```{r}
#| eval: false
shan_sf = left_join(shan_sf, ict_derived, by = c("TS_PCODE" = "TS_PCODE"))

write_rds(shan_sf, "data/rds/shan_sf.rds")
```

```{r}
shan_sf = read_rds("data/rds/shan_sf.rds")
```

Now lets plot it into a choropleth map using *qtm()*

```{r}
qtm(shan_sf, "RADIO_PR")
```

If we were to plot out the household distribution and baseline radio distribution, it will look differently.

```{r}
#| fig-width: 12
tm_shape(shan_sf) +
  tm_polygons(c("RADIO", "RADIO_PR", "TT_HOUSEHOLDS"), style = "jenks")+
  tm_facets(sync = TRUE, ncol = 3) +
  tm_legend(legend.position = c("right", "bottom"))
  tm_layout(outer.margins = 0, asp = 0)
```

We can see that areas with higher amounts of households tend to have more radio, but this does not translate to the number of radios in proportion to the number of households, which is shown by the Radio penetration rate.

## Correlation Analysis

Before cluster analysis, we must ensure that cluster variables are not highly correlated. We can do this using *corrplot.mixed()* to visualise and analyse the correlation.

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
               lower = "ellipse",
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

Through this, we can see Computer and Internet are highly correlated. As such, we should only look at one instead of both when doing cluster analysis.

## Hierarchy Cluster Analysis

Now we should extract the clustering variables into their own data frame. Internet will not be used as we will be using computers instead.

```{r}
#| eval: false
cluster_vars = shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars)
```

Now I shall assign the township name to be the the row label instead of row numbers, then remove the TS.x column.

```{r}
#| eval: false
row.names(cluster_vars) = cluster_vars$"TS.x"
shan_ict = select(cluster_vars, c(2:6))
head(shan_ict)
```

```{r}
#| eval: false
write_rds(shan_ict, "data/rds/shan_ict.rds")
```

```{r}
shan_ict = read_rds("data/rds/shan_ict.rds")
```

### Data Standardisation

Min-Max standardisation using *normalize()*

```{r}
shan_ict.std = normalize(shan_ict)
summary(shan_ict.std)
```

Z-score standardisation using *scale()*

```{r}
shan_ict.z = scale(shan_ict)
describe(shan_ict.z) #describe is used over summary as describe provides standard deviation
```

Let's view the effects of the standardisation

```{r}
#| fig-width: 12
#| fig-height: 4
raw = ggplot(data = ict_derived, aes(x = `RADIO_PR`))+
  geom_histogram(bins = 20, color = "black", fill = "orange")+
  ggtitle("Raw values")

min_max = ggplot(data = as.data.frame(shan_ict.std), aes(x = `RADIO_PR`))+
  geom_histogram(bins = 20, color = "black", fill = "orange")+
  ggtitle("Min-Max")

z_score = ggplot(data = as.data.frame(shan_ict.z), aes(x = `RADIO_PR`))+
  geom_histogram(bins = 20, color = "black", fill = "orange")+
  ggtitle("Z-Score")

ggarrange(raw, min_max, z_score, ncol = 3)
```

We can see that the standardisation has normalised the values more to fit a standard distribution.

Now lets compute the proximity matrix using *dist(). dist()* supports six distance proximity calculations, they are: **euclidean, maximum, manhattan, canberra, binary and minkowski**. The default is *euclidean* proximity matrix.

```{r}
proxmat = dist(shan_ict, method = 'euclidean')
proxmat
```

Now we can use *hclust()* to compute the hierarchical clustering.

*hclust()* employs agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC). We shall use the ward.D method.

```{r}
#| fig-width: 10
hclust_ward = hclust(proxmat, method = 'ward.D')
plot(hclust_ward, cex = 0.6)
```

Now we need to identify the stronger clustering structure. This can be done using *agnes()* function to get the agglomerative coefficient, which measures the amount of clustering structure found. Values closer to 1 suggest strong clustering structure.

```{r}
m = c("average", "single", "complete", "ward")
names(m) = c("average", "single", "complete", "ward")

ac = function(x){
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

We can see that ward seems to be the strongest clustering structure. So now we need to determine the optimal clusters.

There are [three](https://statweb.stanford.edu/~gwalther/gap) commonly used methods to determine the optimal clusters, they are:

-   [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering))

-   [Average Silhouette Method](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub)

-   [Gap Statistic Method](http://www.web.stanford.edu/~hastie/Papers/gap.pdf)

Lets use the gap statistic, which can be computed using *clusGap()*. It compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

```{r}
set.seed(12345)
gap_stat = clusGap(shan_ict, FUN = hcut, nstart = 25, K.max = 10, B = 50)
print(gap_stat, method = "firstmax")
```

Now lets visualise the plot using *fviz_gap_stat()*

```{r}
fviz_gap_stat(gap_stat)
```

Recommended number of clusters to retain is 1, but that is not logical. The second best would be 6 clusters, it gives the largest gap statistic.

```{r}
#| fig-width: 10
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, k = 6, border = 2:5)
```

Each leaf is one observation. Observations that are similar ar combined into branches. The higher the fusion, the less similar the two observations are.

Now lets perform visually-driven hiearchical clustering analysis. First we convert it into a data matrix, then use *heatmaply()* to build an interactive cluster heatmap

```{r}
#| fig-width: 12
#| fig-height: 8
shan_ict_mat = data.matrix(shan_ict)

heatmaply(normalize(shan_ict_mat),
          Colv = NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Oranges,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main = "Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State")
```

Now lets map it. We can use *cutree().*

```{r}
groups = as.factor(cutree(hclust_ward, k = 6)) #Output is a list object
```

As the output is a list object, we need to append it to shan_sf, a simple object.

The code chunk below form the join in three steps:

-   the *groups* list object will be converted into a matrix;

-   *cbind()* is used to append *groups* matrix onto shan_sf to produce an output simple feature object called `shan_sf_cluster`; and

-   *rename* of **dplyr** package is used to rename *as.matrix.groups* field as *CLUSTER*.

```{r}
shan_sf_cluster = cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)

qtm(shan_sf_cluster, "CLUSTER")
```

The choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.

## Spatially Constrained Clustering: SKATER approach

This is how to use *skater()* to derive spatially constrained clusters.

First, it needs to be converted into a **SpatialPolygonsDataFrame** object as SKATER only supports **sp** objects.

```{r}
shan_sp = as_Spatial(shan_sf)
```

Now, we can compute the neighbour list.

```{r}
shan.nb = poly2nb(shan_sp)
summary(shan.nb)
```

Now we can plot this neighbour list.

```{r}
#| fig-width: 10
coords = st_coordinates(st_centroid(st_geometry(shan_sf))) # Provide boundary data for shan.nb plotting
plot(st_geometry(shan_sf), border = grey(.5)) #provides the base map
plot(shan.nb, coords, col = "blue", add = TRUE) # setting add=TRUE plots this plot over the previous map plot
```

Now we can calculate the miminum spanning tree.

We start by computing the cost of each edge using *nbcosts().* This is the distance between each node.

```{r}
lcosts = nbcosts(shan.nb, shan_ict)
```

Each observation gives the dissimilarity between its values and the values for the neighbouring observation. It is the notion of a generalised weight for a spatial weights matrix. Now we can incorporate these costs into a weights object. We use style "B" to ensure that cost values are not row-standardised.

```{r}
shan.w = nb2listw(shan.nb, lcosts, style = "B")
summary(shan.w)
```

Now we can compute the minimum spanning tree (MST) using *mstree().*

```{r}
shan.mst = mstree(shan.w)
class(shan.mst) # Check the class of shan.mst
dim(shan.mst) #see the dimentions of the matrix
```

The tree has 54 rows, as it consists of the n-1 edges needed to traverse all 55 nodes.

Now let us plot the MST to include the observation numbers

```{r}
#| fig-width: 10
plot(st_geometry(shan_sf), border = grey(.5)) #provides the base map
plot.mst(shan.mst, coords, col = "blue", add = TRUE, cex.labels = 0.7, cex.circles = 0) # setting add=TRUE plots this plot over the previous map plot
```

Now we can compute the spatially constrained clusters using *skater().*

```{r}
clust6 = skater(edges = shan.mst[,1:2], #give the first 2 columns of the matrix, ignore the cost (V3)
                data = shan_ict,
                method = "euclidean",
                ncuts = 5) #Give the number of cuts. It is set 1 less than the number of clusters (how many times to cut, 1 cut = 2 pieces)
str(clust6)
```

Now we can see which nodes and edges are clustered together, followed by a summary. The sum of squares measures are given as ssto for the total, and ssw to show the effect of each cuts on the overall criterion. We can print the groups to see which node has been assigned to which cluster, and how many nodes are in each cluster.

```{r}
clust6$groups
table(clust6$groups)
```

Now lets plot it out

```{r}
#| fig-width: 10
plot(st_geometry(shan_sf), border = grey(.5)) #provides the base map
plot(clust6, coords, add = TRUE, cex.labels = 0.7, cex.circles = 0, 
     groups.colors = c("red","green","blue", "brown", "pink", "yellow")) 
```

Now let's plot it into a choropleth map to see the clusters even clearer. We can plot the hierarchical clustering map for comparison.

```{r}
groups_mat = as.matrix(clust6$groups)
shan_sf_spatialcluster = cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)

shclust.map = qtm(shan_sf_spatialcluster, "SP_CLUSTER")
hclust.map = qtm(shan_sf_cluster, "CLUSTER")

tmap_arrange(hclust.map, shclust.map, ncol = 2)
```

## Spatially Constrained Clustering: ClustGeo Method

[**ClustGeo**](https://cran.r-project.org/web/packages/ClustGeo/) package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called `hclustgeo()` including spatial/geographical constraints.

In the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between \[0, 1\]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the **attribute/clustering variable space**. D1, on the other hand, gives the dissimilarities in the **constraint space**. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.

The idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called `choicealpha()`.

Now using *hclustgeo()*, we can perform a typical Ward-like hierarchical clustering, similar to *hclust().* We only need to provide a dissimilarity matrix.

```{r}
#| fig-width: 10
nongeo_cluster = hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, k = 6, border = 2:5)
```

Now let's map the clusters formed.

```{r}
groups = as.factor(cutree(nongeo_cluster, k = 6))
shan_sf_ngeo_cluster = cbind(shan_sf, as.matrix(groups)) %>%
  rename(`NG_CLUSTER` = `as.matrix.groups.`)

ngclust.map = qtm(shan_sf_ngeo_cluster, "NG_CLUSTER")

tmap_arrange(hclust.map, ngclust.map, ncol = 2)
```

We can see there are some differences. This is because we are not doing a spatially constrained hierarchical clustering. To do it, we will need a spatial distance matrix.

```{r}
distmat = as.dist(st_distance(shan_sf, shan_sf))
```

Now we can use *choicealpha()* to determine a suitable value for the mixing parameter alpha.

```{r}
cr = choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K = 6, graph = TRUE)
```

Based on the graphs, we can use 0.2 as the alpha value.

```{r}
clustG = hclustgeo(proxmat, distmat, alpha = 0.2)
groups = as.factor(cutree(clustG, k = 6))

shan_sf_Gcluster = cbind(shan_sf, as.matrix(groups)) %>% 
  rename(`G_CLUSTER` = `as.matrix.groups.`)

gclust.map = qtm(shan_sf_Gcluster, "G_CLUSTER")

tmap_arrange(hclust.map, gclust.map, ncol = 2)
```

## Visual Interpretation of Clusters

We can use boxplot to visualise a single clustering variable.

```{r}
ggplot(data = shan_sf_ngeo_cluster, aes(x = NG_CLUSTER, y = RADIO_PR)) + geom_boxplot()
```

The boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.

We can also do **multivariate visualisation**.

```{r}
#| fig-width: 10
ggparcoord(data = shan_sf_ngeo_cluster,
           columns = c(110:114),
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE,
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster") +
  facet_grid(~ NG_CLUSTER) +
  theme(axis.text.x = element_text(angle = 50))
```

Note that the `scale` argument of `ggparcoor()` provide several methods to scale the clustering variables. They are:

-   std: univariately, subtract mean and divide by standard deviation.

-   robust: univariately, subtract median and divide by median absolute deviation.

-   uniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.

-   globalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.

-   center: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.

-   centerObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param

We can also generate the mean of the different clustering variables.

```{r}
shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(NG_CLUSTER) %>% 
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```
