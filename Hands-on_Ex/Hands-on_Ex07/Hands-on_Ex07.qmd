---
title: "Hands-on Exercise 7"
author: "Dexter Wan"
date: "October 11, 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  freeze: true
---

## Description

Learning to compute Global and Local Measures of Spatial Autocorrelation in R.

### Changelog

11 Oct 24: Completed Hands-on Exercise 7.

## Importing Data and setting up R Environment

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, ggpubr, cluster, factoextra, NbClust, heatmaply, corrplot, psych, tidyverse, GGally)
```

```{r}
#| eval: false
shan_sf = st_read(dsn = "data/geospatial", 
                  layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>% # %in% indicates that we only want certain values in column ST
  select(c(2:7))
shan_sf
```

```{r}
ict = read_csv("data/aspatial/Shan-ICT.csv")
```

```{r}
summary(ict)
```

We will now derive the penetration rate of each ICT variable.

```{r}
ict_derived = ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
summary(ict_derived)
```

## Exploratory Data Analysis (EDA)

EDA is useful to see the distribution of variables.

Histograms can identify the overall distribution (e.g. left skew)

```{r}
ggplot(data = ict_derived, 
       aes(x = `RADIO`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")
```

Boxplot can detect outliers

```{r}
ggplot(data = ict_derived,
       aes(x = `RADIO`)) +
  geom_boxplot(color = "black",
               fill = "orange")
```

We can also do this using the penetration rate

```{r}
ggplot(data = ict_derived, 
       aes(x = `RADIO_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")
```

```{r}
ggplot(data = ict_derived,
       aes(x = `RADIO_PR`)) +
  geom_boxplot(color = "black",
               fill = "orange")
```

We can plot multiple plots side-by-side using *ggarrange()*

```{r}
radio = ggplot(data = ict_derived, 
             aes(x =  `RADIO_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

tv = ggplot(data = ict_derived, 
             aes(x =  `TV_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

llphone = ggplot(data = ict_derived, 
             aes(x =  `LLPHONE_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

mphone = ggplot(data = ict_derived, 
             aes(x =  `MPHONE_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

computer = ggplot(data = ict_derived, 
             aes(x =  `COMPUTER_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")

internet = ggplot(data = ict_derived, 
             aes(x =  `INTERNET_PR`)) +
  geom_histogram(bins = 20, 
                 color = "black", 
                 fill = "orange")
```

```{r}
#| fig-width: 12
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, nrow = 2)
```

Now lets join the data into a single sf file

```{r}
#| eval: false
shan_sf = left_join(shan_sf, ict_derived, by = c("TS_PCODE" = "TS_PCODE"))

write_rds(shan_sf, "data/rds/shan_sf.rds")
```

```{r}
shan_sf = read_rds("data/rds/shan_sf.rds")
```

Now lets plot it into a choropleth map using *qtm()*

```{r}
qtm(shan_sf, "RADIO_PR")
```

If we were to plot out the household distribution and baseline radio distribution, it will look differently.

```{r}
#| fig-width: 12
tm_shape(shan_sf) +
  tm_polygons(c("RADIO", "RADIO_PR", "TT_HOUSEHOLDS"), style = "jenks")+
  tm_facets(sync = TRUE, ncol = 3) +
  tm_legend(legend.position = c("right", "bottom"))
  tm_layout(outer.margins = 0, asp = 0)
```

We can see that areas with higher amounts of households tend to have more radio, but this does not translate to the number of radios in proportion to the number of households, which is shown by the Radio penetration rate.

## Correlation Analysis

Before cluster analysis, we must ensure that cluster variables are not highly correlated. We can do this using *corrplot.mixed()* to visualise and analyse the correlation.

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
               lower = "ellipse",
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

Through this, we can see Computer and Internet are highly correlated. As such, we should only look at one instead of both when doing cluster analysis.

## Hierarchy Cluster Analysis

Now we should extract the clustering variables into their own data frame. Internet will not be used as we will be using computers instead.

```{r}
#| eval: false
cluster_vars = shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars)
```

Now I shall assign the township name to be the the row label instead of row numbers, then remove the TS.x column.

```{r}
#| eval: false
row.names(cluster_vars) = cluster_vars$"TS.x"
shan_ict = select(cluster_vars, c(2:6))
head(shan_ict)
```

```{r}
#| eval: false
write_rds(shan_ict, "data/rds/shan_ict.rds")
```

```{r}
shan_ict = read_rds("data/rds/shan_ict.rds")
```

### Data Standardisation

Min-Max standardisation using *normalize()*

```{r}
shan_ict.std = normalize(shan_ict)
summary(shan_ict.std)
```

Z-score standardisation using *scale()*

```{r}
shan_ict.z = scale(shan_ict)
describe(shan_ict.z) #describe is used over summary as describe provides standard deviation
```

Let's view the effects of the standardisation

```{r}
#| fig-width: 12
#| fig-height: 4
raw = ggplot(data = ict_derived, aes(x = `RADIO_PR`))+
  geom_histogram(bins = 20, color = "black", fill = "orange")+
  ggtitle("Raw values")

min_max = ggplot(data = as.data.frame(shan_ict.std), aes(x = `RADIO_PR`))+
  geom_histogram(bins = 20, color = "black", fill = "orange")+
  ggtitle("Min-Max")

z_score = ggplot(data = as.data.frame(shan_ict.z), aes(x = `RADIO_PR`))+
  geom_histogram(bins = 20, color = "black", fill = "orange")+
  ggtitle("Z-Score")

ggarrange(raw, min_max, z_score, ncol = 3)
```

We can see that the standardisation has normalised the values more to fit a standard distribution.

Now lets compute the proximity matrix using *dist(). dist()* supports six distance proximity calculations, they are: **euclidean, maximum, manhattan, canberra, binary and minkowski**. The default is *euclidean* proximity matrix.

```{r}
proxmat = dist(shan_ict)
proxmat
```

Now we can use *hclust()* to compute the hierarchical clustering.

*hclust()* employs agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC). We shall use the ward.D method.

```{r}
#| fig-width: 10
hclust_ward = hclust(proxmat, method = 'ward.D')
plot(hclust_ward, cex = 0.6)
```

Now we need to identify the stronger clustering structure. This can be done using *agnes()* function to get the agglomerative coefficient, which measures the amount of clustering structure found. Values closer to 1 suggest strong clustering structure.

```{r}
m = c("average", "single", "complete", "ward")
names(m) = c("average", "single", "complete", "ward")

ac = function(x){
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

We can see that ward seems to be the strongest clustering structure. So now we need to determine the optimal clusters.

There are [three](https://statweb.stanford.edu/~gwalther/gap) commonly used methods to determine the optimal clusters, they are:

-   [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering))

-   [Average Silhouette Method](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub)

-   [Gap Statistic Method](http://www.web.stanford.edu/~hastie/Papers/gap.pdf)

Lets use the gap statistic, which can be computed using *clusGap()*. It compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

```{r}
set.seed(12345)
gap_stat = clusGap(shan_ict, FUN = hcut, nstart = 25, K.max = 10, B = 50)
print(gap_stat, method = "firstmax")
```

Now lets visualise the plot using *fviz_gap_stat()*

```{r}
fviz_gap_stat(gap_stat)
```

Recommended number of clusters to retain is 1, but that is not logical. The second best would be 6 clusters, it gives the largest gap statistic.

```{r}
#| fig-width: 10
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, k = 6, border = 2:5)
```

Each leaf is one observation. Observations that are similar ar combined into branches. The higher the fusion, the less similar the two observations are.

Now lets perform visually-driven hiearchical clustering analysis. First we convert it into a data matrix, then use *heatmaply()* to build an interactive cluster heatmap

```{r}
#| fig-width: 12
#| fig-height: 8
shan_ict_mat = data.matrix(shan_ict)

heatmaply(normalize(shan_ict_mat),
          Colv = NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Oranges,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main = "Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State")
```

Now lets map it. We can use *cutree().*

```{r}
groups = as.factor(cutree(hclust_ward, k = 6)) #Output is a list object
```

As the output is a list object, we need to append it to shan_sf, a simple object.

The code chunk below form the join in three steps:

-   the *groups* list object will be converted into a matrix;

-   *cbind()* is used to append *groups* matrix onto shan_sf to produce an output simple feature object called `shan_sf_cluster`; and

-   *rename* of **dplyr** package is used to rename *as.matrix.groups* field as *CLUSTER*.

```{r}
shan_sf_cluster = cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)

qtm(shan_sf_cluster, "CLUSTER")
```

The choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.
